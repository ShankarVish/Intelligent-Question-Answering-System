# ğŸ§  Intelligent Question Answering System (RAG + LLM)

A Retrieval-Augmented Generation (RAG) system using LangChain, OpenAI, and Pinecone to answer questions from PDFs, DOCX, TXT, or Wikipedia articles.

---

## ğŸ”§ Main Features

- âœ… Load files (PDF, DOCX, TXT) or Wikipedia articles
- âœ‚ï¸ Split text into manageable chunks
- ğŸ” Embed chunks using OpenAI (`text-embedding-3-small`)
- ğŸ“¦ Store & retrieve embeddings with Pinecone
- ğŸ’¬ Ask questions using `GPT-3.5-Turbo` (LangChain RAG pipeline)
- ğŸ§  Optional memory for multi-turn chat (ConversationalRetrievalChain)
- ğŸ”„ Interactive Q&A loop in terminal

---

## ğŸ“ Workflow

1. **Load documents**
2. **Chunk text**
3. **Generate embeddings**
4. **Store/fetch from Pinecone**
5. **Ask questions â†’ Get answers**
6. **(Optional)** Maintain conversation history

---

## ğŸ§° Tech Stack

- `LangChain`, `OpenAI`, `Pinecone`
- `PyPDFLoader`, `Docx2txtLoader`, `WikipediaLoader`
- `GPT-3.5-Turbo`, `text-embedding-3-small`
- `.env` config for API keys

---

## â–¶ï¸ Run

```bash
pip install openai langchain pinecone-client tiktoken pypdf docx2txt wikipedia
```

Set API keys in `.env`, then run your script.

---

## ğŸ“Œ Example

```text
Question: What is the Bill of Rights?
Answer: The Bill of Rights refers to the first ten amendments...
```

---

## ğŸ—‘ Cleanup

Delete indexes from Pinecone with:

```python
delete_pinecone_index("your_index")  # or "all"
```

---

